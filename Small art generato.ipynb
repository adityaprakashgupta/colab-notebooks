{"cells":[{"cell_type":"markdown","metadata":{"id":"aHODEET8VqPl"},"source":["## Making things ready"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"77cjVXtPTWhS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n","Thu Feb  3 08:09:35 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","--2022-02-03 08:09:35--  https://v-diffusion.s3.us-west-2.amazonaws.com/512x512_diffusion_uncond_finetune_008100.pt\n","Resolving v-diffusion.s3.us-west-2.amazonaws.com (v-diffusion.s3.us-west-2.amazonaws.com)... 52.218.154.25\n","Connecting to v-diffusion.s3.us-west-2.amazonaws.com (v-diffusion.s3.us-west-2.amazonaws.com)|52.218.154.25|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2232025447 (2.1G) [application/octet-stream]\n","Saving to: â€˜512x512_diffusion_uncond_finetune_008100.ptâ€™\n","\n","512x512_diffusion_u 100%[===================\u003e]   2.08G  21.7MB/s    in 1m 40s  \n","\n","2022-02-03 08:11:16 (21.3 MB/s) - â€˜512x512_diffusion_uncond_finetune_008100.ptâ€™ saved [2232025447/2232025447]\n","\n","Cloning into 'CLIP'...\n","remote: Enumerating objects: 195, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n","Receiving objects: 100% (195/195), 8.91 MiB | 12.37 MiB/s, done.\n","Resolving deltas: 100% (94/94), done.\n","Cloning into 'guided-diffusion'...\n","remote: Enumerating objects: 98, done.\u001b[K\n","remote: Counting objects: 100% (98/98), done.\u001b[K\n","remote: Compressing objects: 100% (67/67), done.\u001b[K\n","remote: Total 98 (delta 52), reused 73 (delta 30), pack-reused 0\u001b[K\n","Unpacking objects: 100% (98/98), done.\n","Obtaining file:///content/CLIP\n","Collecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy-\u003eclip==1.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch-\u003eclip==1.0) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003eclip==1.0) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003eclip==1.0) (7.1.2)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=1fa24815dc1b8b24f547a9b883b883628303e9e482e0bf3bf03d0b586c798ced\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","Successfully built ftfy\n","Installing collected packages: ftfy, clip\n","  Running setup.py develop for clip\n","Successfully installed clip-1.0 ftfy-6.0.3\n","Obtaining file:///content/guided-diffusion\n","Collecting blobfile\u003e=1.0.5\n","  Downloading blobfile-1.2.8-py3-none-any.whl (66 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66 kB 2.5 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (4.62.3)\n","Collecting xmltodict~=0.12.0\n","  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n","Collecting urllib3~=1.25\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138 kB 9.0 MB/s \n","\u001b[?25hCollecting pycryptodomex~=3.8\n","  Downloading pycryptodomex-3.14.0-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0 MB 41.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile\u003e=1.0.5-\u003eguided-diffusion==0.0.0) (3.4.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch-\u003eguided-diffusion==0.0.0) (3.10.0.2)\n","Installing collected packages: xmltodict, urllib3, pycryptodomex, blobfile, guided-diffusion\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Running setup.py develop for guided-diffusion\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed blobfile-1.2.8 guided-diffusion-0.0.0 pycryptodomex-3.14.0 urllib3-1.26.8 xmltodict-0.12.0\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 1.5 MB/s \n","\u001b[?25hCollecting datetime\n","  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.10.0+cu111)\n","Requirement already satisfied: torchvision\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.11.1+cu111)\n","Requirement already satisfied: tqdm\u003e=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n","Requirement already satisfied: scipy\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n","Requirement already satisfied: numpy\u003e=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=0.4.0-\u003elpips) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision\u003e=0.2.1-\u003elpips) (7.1.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datetime) (2018.9)\n","Collecting zope.interface\n","  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251 kB 25.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface-\u003edatetime) (57.4.0)\n","Installing collected packages: zope.interface, lpips, datetime\n","Successfully installed datetime-4.3 lpips-0.1.4 zope.interface-5.4.0\n","Collecting emoji\n","  Downloading emoji-1.6.3.tar.gz (174 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174 kB 4.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=7defb54d3425d1699bc59e33553a3b7799fe8e785d00481a8cd0a007eb690a6d\n","  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.6.3\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"]}],"source":["import torch\n","# Check the GPU status\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","!nvidia-smi\n","!mkdir generations\n","model_path = '/content/'\n","diffusion_model = \"512x512_diffusion_uncond_finetune_008100\"\n","!wget --continue 'https://v-diffusion.s3.us-west-2.amazonaws.com/512x512_diffusion_uncond_finetune_008100.pt'\n","!git clone https://github.com/openai/CLIP\n","!git clone https://github.com/crowsonkb/guided-diffusion\n","!pip install -e ./CLIP\n","!pip install -e ./guided-diffusion\n","!pip install lpips datetime\n","!pip install emoji --upgrade\n","\n","import gc\n","import io\n","import math\n","import sys\n","from IPython import display\n","import lpips\n","from PIL import Image, ImageOps\n","import requests\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as TF\n","from tqdm.notebook import tqdm\n","sys.path.append('./CLIP')\n","sys.path.append('./guided-diffusion')\n","import clip\n","from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n","from datetime import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0SLD-11UUre"},"outputs":[],"source":["def interp(t):\n","    return 3 * t**2 - 2 * t ** 3\n","\n","def perlin(width, height, scale=10, device=None):\n","    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n","    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n","    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n","    wx = 1 - interp(xs)\n","    wy = 1 - interp(ys)\n","    dots = 0\n","    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n","    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n","    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n","    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n","    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n","\n","def perlin_ms(octaves, width, height, grayscale, device=device):\n","    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n","    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n","    for i in range(1 if grayscale else 3):\n","        scale = 2 ** len(octaves)\n","        oct_width = width\n","        oct_height = height\n","        for oct in octaves:\n","            p = perlin(oct_width, oct_height, scale, device)\n","            out_array[i] += p * oct\n","            scale //= 2\n","            oct_width *= 2\n","            oct_height *= 2\n","    return torch.cat(out_array)\n","\n","def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n","    out = perlin_ms(octaves, width, height, grayscale)\n","    if grayscale:\n","        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n","        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n","    else:\n","        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n","        out = TF.resize(size=(side_y, side_x), img=out)\n","        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n","\n","    out = ImageOps.autocontrast(out)\n","    return out\n","\n","def fetch(url_or_path):\n","    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n","        r = requests.get(url_or_path)\n","        r.raise_for_status()\n","        fd = io.BytesIO()\n","        fd.write(r.content)\n","        fd.seek(0)\n","        return fd\n","    return open(url_or_path, 'rb')\n","\n","\n","def parse_prompt(prompt):\n","    if prompt.startswith('http://') or prompt.startswith('https://'):\n","        vals = prompt.rsplit(':', 2)\n","        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n","    else:\n","        vals = prompt.rsplit(':', 1)\n","    vals = vals + ['', '1'][len(vals):]\n","    return vals[0], float(vals[1])\n","\n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n","\n","def lanczos(x, a):\n","    cond = torch.logical_and(-a \u003c x, x \u003c a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n","\n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n","\n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n","\n","    input = input.reshape([n * c, 1, h, w])\n","\n","    if dh \u003c h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n","\n","    if dw \u003c w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n","\n","    input = input.reshape([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n","\n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, skip_augs=False):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.skip_augs = skip_augs\n","        self.augs = T.Compose([\n","            T.RandomHorizontalFlip(p=0.5),\n","            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n","            T.RandomAffine(degrees=15, translate=(0.1, 0.1), interpolation=TF.InterpolationMode.BILINEAR),\n","            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n","            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n","            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n","            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n","            T.RandomGrayscale(p=0.35),\n","        ])\n","\n","    def forward(self, input):\n","        input = T.Pad(input.shape[2]//4, fill=0)(input)\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","\n","        cutouts = []\n","        for ch in range(cutn):\n","            if ch \u003e cutn - cutn//4:\n","                cutout = input.clone()\n","            else:\n","                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n","                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n","                offsety = torch.randint(0, abs(sideY - size + 1), ())\n","                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","\n","            if not self.skip_augs:\n","                cutout = self.augs(cutout)\n","            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","            del cutout\n","\n","        cutouts = torch.cat(cutouts, dim=0)\n","        return cutouts\n","\n","\n","def spherical_dist_loss(x, y):\n","    x = F.normalize(x, dim=-1)\n","    y = F.normalize(y, dim=-1)\n","    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n","\n","\n","def tv_loss(input):\n","    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n","    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n","    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n","    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n","    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n","\n","\n","def range_loss(input):\n","    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n","\n","def unitwise_norm(x, norm_type=2.0):\n","    if x.ndim \u003c= 1:\n","        return x.norm(norm_type)\n","    else:\n","        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n","        # might need special cases for other weights (possibly MHA) where this may not be true\n","        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n","\n","def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n","    if isinstance(parameters, torch.Tensor):\n","        parameters = [parameters]\n","    for p in parameters:\n","        if p.grad is None:\n","            continue\n","        p_data = p.detach()\n","        g_data = p.grad.detach()\n","        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n","        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n","        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n","        new_grads = torch.where(grad_norm \u003c max_norm, g_data, clipped_grad)\n","        p.grad.detach().copy_(new_grads)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"elapsed":43028,"status":"ok","timestamp":1643873384020,"user":{"displayName":"Aditya Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizia8QOEPWlEH7zrCx_bGEx8JAk-hS4ZLviIUuJg=s64","userId":"08497966387154079823"},"user_tz":-330},"id":"xGYYOTKSUUmq","outputId":"60db86d6-798d-4784-efa9-05bc75147550"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:04\u003c00:00, 71.0MiB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ffcb31a6aba443bbe5c96122457b0d0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/528M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"]}],"source":["def regen_perlin():\n","    if perlin_mode == 'color':\n","        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n","        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n","    elif perlin_mode == 'gray':\n","        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n","        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n","    else:\n","        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n","        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n","\n","    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n","    del init2\n","    return init.expand(batch_size, -1, -1, -1)\n","\n","def do_run(x):\n","    loss_values = []\n"," \n","    if seed is not None:\n","        np.random.seed(seed)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # torch.backends.cudnn.deterministic = True\n"," \n","    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs)\n","    target_embeds, weights = [], []\n"," \n","    for prompt in text_prompts:\n","        txt, weight = parse_prompt(prompt)\n","        txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n","        target_embeds.append(txt)\n","        weights.append(weight)\n"," \n","    for prompt in image_prompts:\n","        path, weight = parse_prompt(prompt)\n","        img = Image.open(fetch(path)).convert('RGB')\n","        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n","        batch = make_cutouts(TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n","        embed = clip_model.encode_image(normalize(batch)).float()\n","        target_embeds.append(embed)\n","        weights.extend([weight / cutn] * cutn)\n"," \n","    target_embeds = torch.cat(target_embeds)\n","    weights = torch.tensor(weights, device=device)\n","    if weights.sum().abs() \u003c 1e-3:\n","        raise RuntimeError('The weights must not sum to 0.')\n","    weights /= weights.sum().abs()\n"," \n","    init = None\n","    if init_image is not None:\n","        init = Image.open(fetch(init_image)).convert('RGB')\n","        init = init.resize((side_x, side_y), Image.LANCZOS)\n","        init = TF.to_tensor(init).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device).mul(2).sub(1)\n"," \n","    cur_t = None\n","    def cond_fn(x, t, out, y=None):\n","        n = x.shape[0]\n","        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n","        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n","        x_in_grad = torch.zeros_like(x_in)\n","\n","        for i in range(cutn_batches):\n","            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n","            image_embeds = clip_model.encode_image(clip_in).float()\n","            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n","            dists = dists.view([cutn, n, -1])\n","            losses = dists.mul(weights).sum(2).mean(0)\n","            loss_values.append(losses.sum().item())\n","            x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n","\n","        tv_losses = tv_loss(x_in)\n","        range_losses = range_loss(out['pred_xstart'])\n","        sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n","        loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n","        if init is not None and init_scale:\n","            init_losses = lpips_model(x_in, init)\n","            loss = loss + init_losses.sum() * init_scale\n","        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n","        grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n","        adaptive_clip_grad([x])\n","        magnitude = grad.square().mean().sqrt()\n","        return grad * magnitude.clamp(max=clamp_max) / magnitude\n"," \n","    if model_config['timestep_respacing'].startswith('ddim'):\n","        sample_fn = diffusion.ddim_sample_loop_progressive\n","    else:\n","        sample_fn = diffusion.p_sample_loop_progressive\n"," \n","    original_target_embeds = target_embeds.clone()\n","    for i in range(n_batches):\n","        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n","\n","        if fuzzy_prompt:\n","            target_embeds = original_target_embeds.clone() +  torch.randn_like(target_embeds).cuda() * rand_mag\n","\n","        if perlin_init:\n","            init = regen_perlin()\n"," \n","        if model_config['timestep_respacing'].startswith('ddim'):\n","            samples = sample_fn(\n","                model,\n","                (batch_size, 3, side_y, side_x),\n","                clip_denoised=clip_denoised,\n","                model_kwargs={},\n","                cond_fn=cond_fn,\n","                progress=True,\n","                skip_timesteps=skip_timesteps,\n","                init_image=init,\n","                randomize_class=randomize_class,\n","                eta=eta,\n","                cond_fn_with_grad=True,\n","            )\n","        else:\n","            samples = sample_fn(\n","                model,\n","                (batch_size, 3, side_y, side_x),\n","                clip_denoised=clip_denoised,\n","                model_kwargs={},\n","                cond_fn=cond_fn,\n","                progress=True,\n","                skip_timesteps=skip_timesteps,\n","                init_image=init,\n","                randomize_class=randomize_class,\n","                cond_fn_with_grad=True,\n","            )\n","\n","        for j, sample in enumerate(samples):\n","            # display.clear_output(wait=True)\n","            cur_t -= 1\n","            if j % display_rate == 0 or cur_t == -1:\n","                for k, image in enumerate(sample['pred_xstart']):\n","                    tqdm.write(f'Generated Image {x}, step {j}:')\n","                    current_time = datetime.now().strftime('%H:%M:%S')\n","                    filename = f'generations/generation_{x}.png'\n","                    image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n","                    image.save('/content/' + filename)\n","                    display.display(display.Image('/content/' + filename))\n"," \n","        plt.plot(np.array(loss_values), 'r')\n","\n","  \n","  # timestep_respacing = 'ddim50' # Modify this value to decrease the number of timesteps.\n","timestep_respacing = '50'\n","diffusion_steps = 1000\n","\n","model_config = model_and_diffusion_defaults()\n","if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n","    model_config.update({\n","        'attention_resolutions': '32, 16, 8',\n","        'class_cond': False,\n","        'diffusion_steps': diffusion_steps,\n","        'rescale_timesteps': True,\n","        'timestep_respacing': timestep_respacing,\n","        'image_size': 512,\n","        'learn_sigma': True,\n","        'noise_schedule': 'linear',\n","        'num_channels': 256,\n","        'num_head_channels': 64,\n","        'num_res_blocks': 2,\n","        'resblock_updown': True,\n","        'use_fp16': True,\n","        'use_scale_shift_norm': True,\n","        'use_checkpoint': True,\n","    })\n","elif diffusion_model == '256x256_diffusion_uncond':\n","    model_config.update({\n","        'attention_resolutions': '32, 16, 8',\n","        'class_cond': False,\n","        'diffusion_steps': diffusion_steps,\n","        'rescale_timesteps': True,\n","        'timestep_respacing': timestep_respacing,\n","        'image_size': 256,\n","        'learn_sigma': True,\n","        'noise_schedule': 'linear',\n","        'num_channels': 256,\n","        'num_head_channels': 64,\n","        'num_res_blocks': 2,\n","        'resblock_updown': True,\n","        'use_fp16': True,\n","        'use_scale_shift_norm': True,\n","        'use_checkpoint': True,\n","    })\n","side_x = side_y = model_config['image_size']\n","\n","model, diffusion = create_model_and_diffusion(**model_config)\n","model.load_state_dict(torch.load(f'{model_path}{diffusion_model}.pt', map_location='cpu'))\n","model.requires_grad_(False).eval().to(device)\n","for name, param in model.named_parameters():\n","    if 'qkv' in name or 'norm' in name or 'proj' in name:\n","        param.requires_grad_()\n","if model_config['use_fp16']:\n","    model.convert_to_fp16()\n","\n","clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n","clip_size = clip_model.visual.input_resolution\n","normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n","lpips_model = lpips.LPIPS(net='vgg').to(device)"]},{"cell_type":"markdown","metadata":{"id":"1pYd5YoMXL7G"},"source":["## Art Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0PwzFZbLfcy"},"outputs":[],"source":["#@markdown ## Parameters for The Art Generator \n","text = \"the cyberpunk purple noir wizard by Greg Rutkowski - ArtStation HD\" #@param {type:\"string\"}\n","number_of_images = 2#@param {type: \"number\"}\n","# eta = 0.5#@param {type: \"number\"}\n","text_prompts = [\n","    text\n","]\n","\n","image_prompts = [\n","    # 'mona.jpg',\n","]\n","\n","clip_guidance_scale = 15000 # 5000 (new:15000) - Controls how much the image should look like the prompt.\n","tv_scale = 2500 # 500 (new:2500) - Controls the smoothness of the final output.\n","range_scale = 100 # 100 - Controls how far out of range RGB values are allowed to be.\n","sat_scale = 0 # 0 - Controls how much saturation is allowed. From nshepperd's JAX notebook, though not sure if it's doing anything right now...\n","cutn = 16 # 16 - Controls how many crops to take from the image. Increase for higher quality.\n","cutn_batches = 2 # 2 - Accumulate CLIP gradient from multiple batches of cuts [Can help with OOM errors / Low VRAM]\n","\n","init_image = None # None - URL or local path\n","init_scale = 0 # 0 - This enhances the effect of the init image, a good value is 1000\n","skip_timesteps = 8 # 10 (new:5) - Controls the starting point along the diffusion timesteps\n","\n","# Try this option for random natural-looking noise in place of an init image:\n","perlin_init = True # False - Option to start with random perlin noise\n","perlin_mode = 'mixed' # 'mixed' ('gray', 'color')\n","if init_image is not None: # Can't combine init_image and perlin options\n","  perlin_init = False\n","\n","skip_augs = False # False - Controls whether to skip torchvision augmentations\n","randomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\n","clip_denoised = False # False - Determines whether CLIP discriminates a noisy or denoised image\n","clamp_max = 0.035 # 0.05 (new:0.035)\n","\n","fuzzy_prompt = False # False - Controls whether to add multiple noisy prompts to the prompt losses\n","rand_mag = 0.05 # 0.05 - Controls the magnitude of the random noise\n","eta = 0.5 # 0.5 - DDIM hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WUkcDuqV-JeWweCiHN56mp0vJE4wEuQI"},"id":"LHLiO56OfwgD","outputId":"e8a48a79-c070-4bb7-927d-ec7b8a239c90"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["#@title Generate Art\n","display_rate = 1\n","n_batches = 1 # 1 - Controls how many consecutive batches of images are generated\n","batch_size = 1 # 1 - Controls how many images are generated in parallel in a batch\n","# number_of_images = 5\n","\n","# seed = 0\n","# seed = random.randint(0, 2**32) # Choose a random seed and print it at end of run for reproduction\n","for x in range(0, number_of_images):\n","  # display.clear_output(wait=True)\n","  seed = random.randint(0, 2**32)\n","  try:\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","      do_run(x)\n","  except KeyboardInterrupt:\n","      pass\n","  finally:\n","      print('seed', seed)\n","      gc.collect()\n","      torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"4sjiHy2ygOJX"},"outputs":[],"source":["#@title Plot Generated Images! ðŸŽ¯\n","import glob\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","%matplotlib inline\n","\n","images = []\n","for img_path in glob.glob('./generations/*.png'):\n","    images.append(mpimg.imread(img_path))\n","\n","plt.figure(figsize=(50,50))\n","columns = 5\n","for i, image in enumerate(images):\n","    plt.subplot(len(images) / columns + 1, columns, i + 1)\n","    plt.imshow(image)\n","    plt.xticks([])\n","    plt.yticks([])"]},{"cell_type":"markdown","metadata":{"id":"4k_7QguF2PPV"},"source":["#ðŸŒŸðŸŒŸðŸŒŸ **Beautiful Prompts you should try!** ðŸŒŸðŸŒŸðŸŒŸ\n","    the universe is a glitch by greg rutkowski\n","    a space nebula rendered in Cinema4D\n","    revolution of the souls, vector art\n","    the rise of consciousness in the style of WPAP\n","    a vaporwave dragon breathing fire by ross tran\n","    \"Cognitive Transcendence\", matte painting trending on artstation\n","    mirror of love, by greg rutkowski and james jean\n","    falling forever through a bottomless abyss speckled with stars. painting by greg rutkowski\n","    a beautiful epic fantasy painting of a giant robot\n","    the fire of the mind by Ross Tran\n","    the first day of the heavens! trending on artstation\n","    Garden of Hesperides by ArtStation\n","    a beautiful epic wondrous fantasy painting of the ocean\n","    a beautiful watercolor painting of wind\n","    a tropical landscape by Ivan Aivazovsky\n","    a dramatic mountainous landscape by Ivan Aivazovsky\n","    the aurora at night by Ivan Aivazovsky\n","    a painting of a witch brewing a Halloween potion by Greg Rutkowski\n","    a surreal wizards tower by Casper David\n","    a beautiful epic fantasy painting of a giant robot\n","    a rainy city street in the style of cyberpunk noir\n","    the Tower of Babel by Thomas Kinkade\n","    wings of angelic fire in the darkness, trending on artstation\n","    a beautiful fantasy land forest, trending on ArtStation\n","    a desert landscape by Ivan Aivazovsky\n","    a heavenly cloud city by Greg Rutkowski\n","    wings of angelic fire in the darkness, trending on artstation\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNReKKux0KSy4O2h/3hbiLj","collapsed_sections":[],"name":"Small art generato.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01c075f41e164530aed2007244527fb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"073f363b2e2c41b9b4fe29fac2d06297":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a52389c6253450eafc5c0479836cd07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12ba2aa25a094702b2d87708ea98d26a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7e1fd2bb63d46398d300a172848a7cb","IPY_MODEL_4a4b3fbfca71476c8400aa994dd4bc7b","IPY_MODEL_4f15af134aa04af39eb4ed5af290549e"],"layout":"IPY_MODEL_1e7f7a08450c41f5b10265080e1ce50e"}},"1df617ce25514679b46749f77bb251cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e7f7a08450c41f5b10265080e1ce50e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a50b3f054d74734b67e320eed4a33e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bb492975de743cf9a4d456ed79f2ecf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d89338fcfb747aca7850da5b8cf0e8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4779356fd0724eedac8de5cdf0cb01b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb74b19ec2b3499380157627b9b87839","placeholder":"â€‹","style":"IPY_MODEL_9291f4ca98784f6089ad4649ec71dae5","value":"100%"}},"4a4b3fbfca71476c8400aa994dd4bc7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac4b3992b1074a52a0613fbbfce333c3","max":42,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01c075f41e164530aed2007244527fb9","value":2}},"4bf45c307f3c469ba3c8130272e13c80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f15af134aa04af39eb4ed5af290549e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df617ce25514679b46749f77bb251cf","placeholder":"â€‹","style":"IPY_MODEL_8c2dca91c2b44606b52d2bc25af9ecef","value":" 2/42 [00:40\u0026lt;13:30, 20.27s/it]"}},"4ffcb31a6aba443bbe5c96122457b0d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4779356fd0724eedac8de5cdf0cb01b0","IPY_MODEL_5ee9a0b96e384b19803420e9797a1fb1","IPY_MODEL_c7ff171ffc914da8aa580b2c8e6cb734"],"layout":"IPY_MODEL_0a52389c6253450eafc5c0479836cd07"}},"5ee9a0b96e384b19803420e9797a1fb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_790caec24dc24ea8aa76eec8d0f1df23","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a50b3f054d74734b67e320eed4a33e9","value":553433881}},"790caec24dc24ea8aa76eec8d0f1df23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c2dca91c2b44606b52d2bc25af9ecef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9291f4ca98784f6089ad4649ec71dae5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac4b3992b1074a52a0613fbbfce333c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7e1fd2bb63d46398d300a172848a7cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d89338fcfb747aca7850da5b8cf0e8f","placeholder":"â€‹","style":"IPY_MODEL_073f363b2e2c41b9b4fe29fac2d06297","value":"  5%"}},"c7ff171ffc914da8aa580b2c8e6cb734":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb492975de743cf9a4d456ed79f2ecf","placeholder":"â€‹","style":"IPY_MODEL_4bf45c307f3c469ba3c8130272e13c80","value":" 528M/528M [00:04\u0026lt;00:00, 135MB/s]"}},"fb74b19ec2b3499380157627b9b87839":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}