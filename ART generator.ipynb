{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ART generator.ipynb","provenance":[],"authorship_tag":"ABX9TyNHP8Ea14RzIZMZsXIDnLL8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RFFQzUHPZOKs"},"outputs":[],"source":["#@title Check GPU\n","!nvidia-smi"]},{"cell_type":"code","source":["#@title Download Required Python Packages\n"," \n","print(\"Download CLIP...\")\n","!git clone https://github.com/openai/CLIP                 &> /dev/null\n"," \n","print(\"Installing VQGAN...\")\n","!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n","!pip install kornia                                       &> /dev/null\n","!pip install einops                                       &> /dev/null\n","!pip install wget                                         &> /dev/null\n"," \n","print(\"Installing Extra Libraries...\")\n","!pip install stegano                                      &> /dev/null\n","!apt install exempi                                       &> /dev/null\n","!pip install python-xmp-toolkit                           &> /dev/null\n","!pip install imgtag                                       &> /dev/null\n","!pip install pillow==7.1.2                                &> /dev/null\n"," \n","!pip install imageio-ffmpeg                               &> /dev/null\n","!mkdir steps\n","print(\"Installing Finished!!\")"],"metadata":{"cellView":"form","id":"efIA4gqzZaSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Download model\n","!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n","!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384"],"metadata":{"cellView":"form","id":"uwvtUX4yZaKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Make Neural Network\n"," \n","import argparse\n","import math\n","from pathlib import Path\n","import sys\n"," \n","sys.path.append('./taming-transformers')\n","from IPython import display\n","from base64 import b64encode\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n"," \n","from CLIP import clip\n","import kornia.augmentation as K\n","import numpy as np\n","import imageio\n","from PIL import ImageFile, Image\n","from imgtag import ImgTag    # metadatos \n","from libxmp import *         # metadatos\n","import libxmp                # metadatos\n","from stegano import lsb\n","import json\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"," \n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n"," \n"," \n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n"," \n"," \n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n"," \n"," \n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n"," \n","    input = input.view([n * c, 1, h, w])\n"," \n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n"," \n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n"," \n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n"," \n"," \n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n"," \n"," \n","replace_grad = ReplaceGrad.apply\n"," \n"," \n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n"," \n"," \n","clamp_with_grad = ClampWithGrad.apply\n"," \n"," \n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n"," \n"," \n","class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n"," \n","    def forward(self, input):\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n"," \n"," \n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n"," \n"," \n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","        self.augs = nn.Sequential(\n","            K.RandomHorizontalFlip(p=0.5),\n","            # K.RandomSolarize(0.01, 0.01, p=0.7),\n","            K.RandomSharpness(0.3,p=0.4),\n","            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n","            K.RandomPerspective(0.2,p=0.4),\n","            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n","        self.noise_fac = 0.1\n"," \n"," \n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        for _ in range(self.cutn):\n","            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            offsetx = torch.randint(0, sideX - size + 1, ())\n","            offsety = torch.randint(0, sideY - size + 1, ())\n","            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch\n"," \n"," \n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        print(config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n"," \n"," \n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)\n","\n","def download_img(img_url):\n","    try:\n","        return wget.download(img_url,out=\"input.jpg\")\n","    except:\n","        return\n"],"metadata":{"cellView":"form","id":"yl_NXeUfaRRh","executionInfo":{"status":"ok","timestamp":1643544074276,"user_tz":-330,"elapsed":604,"user":{"displayName":"Aditya Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizia8QOEPWlEH7zrCx_bGEx8JAk-hS4ZLviIUuJg=s64","userId":"08497966387154079823"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["#@title Art Generator Parameters\n","text = \"monster world detailed matte painting\" #@param {type:\"string\"}\n","height =  500#@param {type:\"number\"}\n","width =  500#@param {type:\"number\"}\n","ancho=width\n","alto=height\n","model = \"vqgan_imagenet_f16_16384\" #@param [\"vqgan_imagenet_f16_16384\"]\n","interval_image =  50#@param {type:\"number\"}\n","initial_image = \"\"#@param {type:\"string\"}\n","objective_image = \"\"#@param {type:\"string\"}\n","seed = -1#@param {type:\"number\"}\n","max_iterations = -1#@param {type:\"number\"}\n","input_images = \"\"\n","\n","name_models={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384'}\n","name_model = name_models[model]     \n","\n","\n","if seed == -1:\n","    seed = None\n","if initial_image == \"None\":\n","    initial_image = None\n","elif initial_image and initial_image.lower().startswith(\"http\"):\n","    initial_image = download_img(initial_image)\n","\n","\n","if objective_image == \"None\" or not objective_image:\n","    objective_image = []\n","else:\n","    objective_image = objective_image.split(\"|\")\n","    objective_image = [image.strip() for image in objective_image]\n","\n","if initial_image or objective_image != []:\n","    input_images = True\n","\n","text = [frase.strip() for frase in text.split(\"|\")]\n","if text == ['']:\n","    text = []\n","\n","\n","args = argparse.Namespace(\n","    prompts=text,\n","    image_prompts=objective_image,\n","    noise_prompt_seeds=[],\n","    noise_prompt_weights=[],\n","    size=[ancho, alto],\n","    init_image=initial_image,\n","    init_weight=0.,\n","    clip_model='ViT-B/32',\n","    vqgan_config=f'{model}.yaml',\n","    vqgan_checkpoint=f'{model}.ckpt',\n","    step_size=0.1,\n","    cutn=64,\n","    cut_pow=1.,\n","    display_freq=interval_image,\n","    seed=seed,\n",")"],"metadata":{"cellView":"form","id":"4FbhtqWUaRNG","executionInfo":{"status":"ok","timestamp":1643540015217,"user_tz":-330,"elapsed":801,"user":{"displayName":"Aditya Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizia8QOEPWlEH7zrCx_bGEx8JAk-hS4ZLviIUuJg=s64","userId":"08497966387154079823"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#@title Start the Art Generator\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","if text:\n","    print('Using texts:', text)\n","if objective_image:\n","    print('Using image prompts:', objective_image)\n","if args.seed is None:\n","    seed = torch.seed()\n","else:\n","    seed = args.seed\n","torch.manual_seed(seed)\n","print('Using seed:', seed)\n","\n","model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","\n","cut_size = perceptor.visual.input_resolution\n","e_dim = model.quantize.e_dim\n","\n","f = 2**(model.decoder.num_resolutions - 1)\n","make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","n_toks = model.quantize.n_e\n","\n","toksX, toksY = args.size[0] // f, args.size[1] // f\n","sideX, sideY = toksX * f, toksY * f\n","\n","z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","if args.init_image:\n","    pil_image = Image.open(args.init_image).convert('RGB')\n","    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n","    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n","else:\n","    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","    z = one_hot @ model.quantize.embedding.weight\n","    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","z_orig = z.clone()\n","z.requires_grad_(True)\n","opt = optim.Adam([z], lr=args.step_size)\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                 std=[0.26862954, 0.26130258, 0.27577711])\n","\n","pMs = []\n","\n","for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for prompt in args.image_prompts:\n","    path, weight, stop = parse_prompt(prompt)\n","    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n","    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","    embed = perceptor.encode_image(normalize(batch)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","def synth(z):\n","    z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","    \n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","def add_xmp_data(nombrefichero):\n","    imagen = ImgTag(filename=nombrefichero)\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    if args.prompts:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    else:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', name_model, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    #for frases in args.prompts:\n","    #    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'Prompt' ,frases, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.close()\n","\n","def add_stegano_data(filename):\n","    data = {\n","        \"title\": \" | \".join(args.prompts) if args.prompts else None,\n","        \"notebook\": \"VQGAN+CLIP\",\n","        \"i\": i,\n","        \"model\": name_model,\n","        \"seed\": str(seed),\n","        \"input_images\": input_images\n","    }\n","    lsb.hide(filename, json.dumps(data)).save(filename)\n","\n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z)\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')\n","    add_stegano_data('progress.png')\n","    add_xmp_data('progress.png')\n","    display.display(display.Image('progress.png'))\n","\n","def ascend_txt():\n","    global i\n","    out = synth(z)\n","    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","\n","    result = []\n","\n","    if args.init_weight:\n","        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    filename = f\"steps/{i:04}.png\"\n","    imageio.imwrite(filename, np.array(img))\n","    add_stegano_data(filename)\n","    add_xmp_data(filename)\n","    return result\n","\n","def train(i):\n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","    loss = sum(lossAll)\n","    loss.backward()\n","    opt.step()\n","    with torch.no_grad():\n","        z.copy_(z.maximum(z_min).minimum(z_max))\n","\n","i = 0\n","try:\n","    with tqdm() as pbar:\n","        while True:\n","            train(i)\n","            if i == max_iterations:\n","                break\n","            i += 1\n","            pbar.update()\n","except KeyboardInterrupt:\n","    pass"],"metadata":{"cellView":"form","id":"coS_zewKdA6P","executionInfo":{"status":"ok","timestamp":1643542206324,"user_tz":-330,"elapsed":7,"user":{"displayName":"Aditya Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizia8QOEPWlEH7zrCx_bGEx8JAk-hS4ZLviIUuJg=s64","userId":"08497966387154079823"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#@title Optional for specific picture\n","image_number =  101#@param{\"type\": \"integer\"}\n","i = image_number +1"],"metadata":{"cellView":"form","id":"xNpMMFhVtHHY","executionInfo":{"status":"ok","timestamp":1643543984475,"user_tz":-330,"elapsed":908,"user":{"displayName":"Aditya Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizia8QOEPWlEH7zrCx_bGEx8JAk-hS4ZLviIUuJg=s64","userId":"08497966387154079823"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["# Enhance Image & download"],"metadata":{"id":"v_q7slcFhGNj"}},{"cell_type":"code","source":["#@title Download Prerequisites\n","!git clone https://github.com/xinntao/Real-ESRGAN.git    &> /dev/null\n","%cd Real-ESRGAN\n","# Set up the environment\n","!pip install basicsr                                     &> /dev/null\n","!pip install facexlib                                    &> /dev/null\n","!pip install gfpgan                                      &> /dev/null\n","!pip install -r requirements.txt                         &> /dev/null\n","!python setup.py develop                                 &> /dev/null\n","# Download the pre-trained model\n","!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models          &> /dev/null\n","%cd ..\n","torch.cuda.empty_cache()"],"metadata":{"cellView":"form","id":"9g9ra21kiw46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Run image enhancer\n","%cd Real-ESRGAN\n","import os\n","import shutil\n","upload_folder = 'upload'\n","\n","result_folder = 'results'\n","\n","if os.path.isdir(upload_folder):\n","    shutil.rmtree(upload_folder)\n","if os.path.isdir(result_folder):\n","    shutil.rmtree(result_folder)\n","os.mkdir(upload_folder)\n","os.mkdir(result_folder)\n","filename = f'{i-1:04d}.png'\n","\n","dst_path = os.path.join(upload_folder, filename)\n","file_path = os.path.join('/content', 'steps', filename)\n","shutil.copy(file_path, dst_path)\n","\n","!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 4 --half --face_enhance\n","# !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n","%cd .."],"metadata":{"cellView":"form","id":"69DQBAJ0jix1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title # Compare images\n","# utils for visualization\n","import cv2\n","import matplotlib.pyplot as plt\n","def display(img1, img2):\n","  fig = plt.figure(figsize=(25, 10))\n","  ax1 = fig.add_subplot(1, 2, 1) \n","  plt.title('Input image', fontsize=16)\n","  ax1.axis('off')\n","  ax2 = fig.add_subplot(1, 2, 2)\n","  plt.title('Real-ESRGAN output', fontsize=16)\n","  ax2.axis('off')\n","  ax1.imshow(img1)\n","  ax2.imshow(img2)\n","def imread(img_path):\n","  img = cv2.imread(img_path)\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  return img\n","\n","# display each image in the upload folder\n","import os\n","import glob\n","\n","input_folder = 'Real-ESRGAN/upload'\n","result_folder = 'Real-ESRGAN/results'\n","input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n","output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n","for input_path, output_path in zip(input_list, output_list):\n","  img_input = imread(input_path)\n","  img_output = imread(output_path)\n","  display(img_input, img_output)"],"metadata":{"cellView":"form","id":"iC5MTc_Rl6jR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Download Image\n","from google.colab import files\n","\n","filename= os.listdir('/content/Real-ESRGAN/results')\n","files.download('/content/Real-ESRGAN/results/'+filename[0])"],"metadata":{"cellView":"form","id":"HkUAGA7edAzy"},"execution_count":null,"outputs":[]}]}